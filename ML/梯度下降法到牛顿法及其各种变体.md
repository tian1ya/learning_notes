> 学习这些优化算法主要是在读spark 的LogisticRegression 的源码的时候，涉及到这些优化算法，查阅各种资料，然后在这里做一部分总结。

文字都是从博客上学习得来：https://blog.csdn.net/acdreamers/category_3055457.html

---

<img src="./pic/LRGD1.png" alt="recallAda" style="zoom:150%;" />

<img src="./pic/LRGD2.png" alt="recallAda" style="zoom:150%;" />

<img src="./pic/LRGD3.png" alt="recallAda" style="zoom:150%;" />

<img src="./pic/LRGD4.png" alt="recallAda" style="zoom:150%;" />

根据上述公式，只需初始化向量![img](http://images.cnitblog.com/blog/571227/201411/201653081872772.png)全为零，或者随机值，迭代到指定精度为止。

然后上述公式的C++ 的实现博客中也有贴出来：

https://blog.csdn.net/ACdreamers/article/details/44657979

---

* 牛顿法

在该node 的目录下，还有一个牛顿法的pdf 资料。

<img src="./pic/niudun1.png" alt="recallAda" style="zoom:150%;" />

<img src="./pic/niudun2.png" alt="recallAda" style="zoom:150%;" />

<img src="./pic/niudun3.png" alt="recallAda" style="zoom:150%;" />

<img src="./pic/niudun4.png" alt="recallAda" style="zoom:150%;" />

<img src="./pic/niudun5.png" alt="recallAda" style="zoom:150%;" />

<img src="./pic/niudun5.png" alt="recallAda" style="zoom:150%;" />

https://blog.csdn.net/ACdreamers/article/details/44658249

---

* **BFGS算法**。BFGS算法被认为是数值效果最好的拟牛顿

并且具有全局收敛性和超线性收敛速度。那么接下来将会详细讲解。

**Contents**

1. 什么是拟牛顿法

2. 拟牛顿法原理

3. DFP算法原理

4. BFGS算法原理

5. BFGS算法的实现

 

**什么是拟牛顿法**

> 前面Logisitc回归讲解中，我介绍了牛顿法。牛顿法的特点是：收敛速度快，迭代次数少，但是当Hessian
>
> 矩阵很稠密时，每次迭代的计算量很大。随着数据规模的增大，那么Hessian矩阵会越大，需要的存储空间会
>
> 增多，计算量也会增大，有时候大到不可计算，所以针对海量数据的计算，牛顿法不再适用。
>
>    拟牛顿法是在牛顿法的基础上引入了Hessian矩阵的近似矩阵，避免每次迭代都计算Hessian矩阵的逆，它
>
>    的收敛速度介于梯度下降法和牛顿法之间。拟牛顿法跟牛顿法一样，也是不能处理太大规模的数据，因为计算量和存储空间会开销很多。拟牛顿法虽然每次迭代不像牛顿法那样保证是最优化的方向，但是近似矩阵始终是正定的，因此算法始终是朝着最优化的方向在搜索。
>

**拟牛顿法原理**

<img src="./pic/niniudun1.png" alt="recallAda" style="zoom:150%;" />

<img src="./pic/niniudun2.png" alt="recallAda" style="zoom:150%;" />

上式子中B矩阵还是Hessian 举证的逆矩阵，但是因为有H左边的那个一阶迭代式子，使得B 矩阵可以通过迭代的方式计算出来。

* DFP算法原理

<img src="./pic/bfgs.png" alt="recallAda" style="zoom:150%;" />

---

* L-BFGS算法

> **BFGS算法**，它是用来求解最优化问题的，在这个算法中，相对于普通的牛顿迭代法有很大的改进。BFGS算法中，仍然有缺
>
> 陷，比如当优化问题规模很大时，矩阵![img](https://img-blog.csdn.net/20150329163424887)的存储和计算将变得不可行。为了解决这个问题，就有了**L-BFGS算法**。

**L-BFGS算法介绍**

> L-BFGS即Limited-memory BFGS，在之前的BFGS算法中，我们可以不存储矩阵，而是存储最近次迭代
>
>    的曲率信息，即和。当完成一次迭代后，最旧的一次曲率的信息将被删除，而最新的曲率将被保存下来，所
>
>    以这样就保证了保存的曲率信息始终都来自最近的次迭代。在实际工程中取3到20之间的值效果比较好。

**L-BFGS算法原理**

<img src="./pic/bfgs2.png" alt="recallAda" style="zoom:150%;" />

<img src="./pic/bfgs3.png" alt="recallAda" style="zoom:150%;" />

https://blog.csdn.net/acdreamers/article/details/44728041

---

以上的几种优化算法：

1. GD 普通的优化算法，梯度方向增加最快，那么梯度方向的反方向是下降最快的，所以在梯度上加负号保证了一定是下降的，只使用到损失函数的一阶梯度
2. 牛顿法：使用到损失函数的二阶梯度，优化的更快，

**OWL-QN**

主要是针对L1-norm不可微提出的，它是基于这样一个事实：也就是对含有l1 正则的损失函数进行优化，spark也是这么做的，使用l1 正则使用OWL-QN优化，其他的使用L-BFGS方法优化

