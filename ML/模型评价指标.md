* 准确率(Accuracy)
* 精准率(Precision)
* 召回率(Recall)
* F1-score
* 混淆矩阵
* ROC/AUC

---

### 准确率(Accuracy)

> 定义： 预测正确的结果占总样本的百分比

![accuracy](./pic/accuracy.jpeg)

> TP: True Positive, 被模型预测为正的正样本
>
> TN： True Negtive: 被模型预测为负的负样本
>
> FP：False Positive:  被模型预测为负的正样本
>
> FN： False Negtive: 被模型预测为正的负样本

> 这里他有一个致命的缺点，那就是在数据不均衡的时候，纸指标收到很大的影响。

---

### 精准率(Precision)和召回率(Recall)

> 定义： 精准率又叫查准率，针对预测结果而言，在所有预测为正的样本中实际为正的样本的概率。也就是在所有预测为正的结果中，有多少把握可以预测正确。

![accuracy](./pic/Precision.jpeg)



> 定义：召回率，又叫查全率，是针对原有样本而言，在实际为正的的样本中，有多少被预测为正

![accuracy](./pic/Recall.jpeg)

在不同的应用场景下，我们的关注点不同，例如，在预测股票的时候，我们更关心精准率，即我们预测升的那些股票里，真的升了有多少，因为那些我们预测升的股票都是我们投钱的。而在预测病患的场景下，我们更关注召回率，即真的患病的那些人里我们预测错了情况应该越少越好。

精确率和召回率是一对此消彼长的度量。例如在推荐系统中，我们想让推送的内容尽可能用户全都感兴趣，那只能推送我们把握高的内容(我觉得用户喜欢什么)，这样就漏掉了一些用户感兴趣的内容，召回率就低了(没有站在用户的角度)；如果想让用户感兴趣的内容都被推送，那只有将所有内容都推送上，宁可错杀一千，不可放过一个，这样准确率就很低了。

---

### F1-Score

> Precision 和 Recall 是此消彼长的，即精准率高了，召回率就下降，在一些场景下要兼顾精准率和召回率，那就是F-Mesure， 是P 和 R 的加权调和平均

![accuracy](./pic/FMesure.jpeg)

β=1时，也就是常见的F1-Score，是P和R的调和平均，当F1较高时，模型的性能越好。

![accuracy](./pic/FMesure2.jpeg)

---

### ROC 曲线

> ROC曲线有个很好的特性： **当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。**在实际的数据集中经常会出现类别不平衡（Class Imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化，ROC以及AUC可以很好的消除样本类别不平衡对指标结果产生的影响。
>
> ROC是一种不依赖于阈值（Threshold）的评价指标，在输出为概率分布的分类模型中，如果仅使用准确率、精确率、召回率作为评价指标进行模型对比时，都必须时基于某一个给定阈值的，对于不同的阈值，各模型的Metrics结果也会有所不同，这样就很难得出一个很置信的结果。

> 先介绍两个指标， **这两个指标的选择使得ROC可以无视样本的不平衡。**这两个指标分别是： **灵敏度（sensitivity）**和 **特异度（specificity）**，也叫做 **真正率（TPR）**和 **假正率（FPR）**，具体公式如下：

* 真正率(True Positive Rate , **TPR**)，又称灵敏度：

![accuracy](./pic/TPR.jpeg)

灵敏度和召回率是一模一样的，只是名字换了而已

* 假负率(False Negative Rate , **FNR**) ：

![accuracy](./pic/FNR.jpeg)

* 假正率(False Positive Rate , **FPR**) ：

![accuracy](./pic/FPR.jpeg)

* 真负率(True Negative Rate , **TNR**)，又称特异度：

![accuracy](./pic/TNR.jpeg)

> 灵敏度（真正率）TPR是正样本的召回率，特异度（真负率）TNR是负样本的召回率，而假负率FNR=1-TPR、假正率FPR=1-TNR，上述四个量都是针对单一类别的预测结果而言的，所以对整体样本是否均衡并不敏感。举个例子：假设总样本中，90%是正样本，10%是负样本。在这种情况下我们如果使用准确率进行评价是不科学的，但是用TPR和TNR却是可以的，因为TPR只关注90%正样本中有多少是被预测正确的，而与那10%负样本毫无关系，同理，FPR只关注10%负样本中有多少是被预测错误的，也与那90%正样本毫无关系。这样就避免了样本不平衡的问题。

>  ROC（Receiver Operating Characteristic）曲线**，又称接受者操作特征曲线。该曲线最早应用于雷达信号检测领域，用于区分信号与噪声。后来人们将其用于评价模型的预测能力。ROC曲线中的主要两个指标就是 **真正率TPR**和 **假正率FPR**，上面已经解释了这么选择的好处所在。其中横坐标为假正率（FPR），纵坐标为真正率（TPR），下面就是一个标准的ROC曲线图。

ROC曲线也是通过 **遍历所有阈值**来绘制整条曲线的。如果我们不断的遍历所有阈值，预测的正样本和负样本是在不断变化的，相应的在ROC曲线图中也会沿着曲线滑动。

![accuracy](./pic/auc.gif)

我们看到改变阈值只是不断地改变预测的正负样本数，即TPR和FPR，但是曲线本身并没有改变。这是有道理的，阈值并不会改变模型的性能。

---

**判断模型性能**

FPR表示模型对于负样本误判的程度，而TPR表示模型对正样本召回的程度。我们所希望的当然是：负样本误判的越少越好，正样本召回的越多越好。所以总结一下就是 **TPR越高，同时FPR越低（即ROC曲线越陡），那么模型的性能就越好。**参考如下动态图进行理解。

![accuracy](./pic/auc2.gif)

**进行模型的性能比较时，与PR曲线类似，若一个模型A的ROC曲线被另一个模型B的ROC曲线完全包住，则称B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。**

AUC(Area Under Curve)又称为曲线下面积，是处于ROC Curve下方的那部分面积的大小。上文中我们已经提到，对于ROC曲线下方面积越大表明模型性能越好，于是AUC就是由此产生的评价指标。通常，AUC的值介于0.5到1.0之间，较大的AUC代表了较好的Performance。如果模型是完美的，那么它的AUC = 1，证明所有正例排在了负例的前面，如果模型是个简单的二类随机猜测模型，那么它的AUC = 0.5，如果一个模型好于另一个，则它的曲线下方面积相对较大，对应的AUC值也会较大。

---

### **混淆矩阵**

> 混淆矩阵又被称为错误矩阵，通过它可以直观地观察到算法的效果。它的每一列是样本的预测分类，每一行是样本的真实分类（反过来也可以），顾名思义，它反映了分类结果的混淆程度。混淆矩阵i行j列的原始是原本是类别i却被分为类别j的样本个数，计算完之后还可以对之进行可视化：

![accuracy](./pic/confusionMetrix.jpeg)

#### 多分类问题

>  对于多分类问题，或者在二分类问题中，我们有时候会有多组混淆矩阵，例如：多次训练或者在多个数据集上训练的结果，那么估算全局性能的方法有两种，分为宏平均（macro-average）和微平均（micro-average）。简单理解，宏平均就是先算出每个混淆矩阵的P值和R值，然后取得平均P值macro-P和平均R值macro-R，再算出Fβ或F1，而微平均则是计算出混淆矩阵的平均TP、FP、TN、FN，接着进行计算P、R，进而求出Fβ或F1。其它分类指标同理，均可以通过宏平均/微平均计算得出。

![accuracy](./pic/macroMircoAverage.jpeg)

> 需要注意的是，在多分类任务场景中，如果非要用一个综合考量的metric的话， **宏平均会比微平均更差一些**，因为宏平均受稀有类别影响更大。宏平均平等对待每一个类别，所以它的值主要受到稀有类别的影响，而微平均平等考虑数据集中的每一个样本，所以它的值受到常见类别的影响比较大。

http://blog.itpub.net/29829936/viewspace-2655826/

---

#### 多分类任务的 precision、recall、F1

在多分类任务中计算 precsion、recall、F1 的时候，通常是将 n 个类看做 n 个二分类任务来计算。以三分类任务为例，我们可以画出这样一个混淆矩阵：

|                   | label = class_A | label = class_B | label = class_C |
| :---------------: | :-------------: | :-------------: | :-------------: |
| predict = class_A |                 |                 |                 |
| predict = class_B |                 |                 |                 |
| predict = class_C |                 |                 |                 |

这时候需要对每个类单独处理填表。对于 `class_A` 类：

|                            | label = A | label = B or label = C |
| :------------------------: | :-------: | :--------------------: |
|        predict = A         |   TP(A)   |         FP(A)          |
| predict = B or predict = C |   FN(A)   |         TN(A)          |

对于 `class_B` 类：

|                            | label = B | label = A or label = C |
| :------------------------: | :-------: | :--------------------: |
|        predict = B         |   TP(B)   |         FP(B)          |
| predict = A or predict = C |   FN(B)   |         TN(B)          |

对于 `class_C` 类：

|                            | label = C | label = A or label = B |
| :------------------------: | :-------: | :--------------------: |
|        predict = C         |   TP(C)   |         FP(C)          |
| predict = A or predict = B |   FN(C)   |         TN(C)          |

> 这样就可以计算各类的 TP、FP、TN、FN 的值。

> Macro F1: 将n分类的评价拆成n个二分类的评价，计算每个二分类的F1 score，n个F1 score的平均值即为Macro F1。 Micro F1: 将n分类的评价拆成n个二分类的评价，将n个二分类评价的TP、FP、RN对应相加，计算评价准确率和召回率，由这2个准确率和召回率计算的F1 score即为Micro F1。 一般来讲，Macro F1、Micro F1 高的分类效果好。Macro F1受样本数量少的类别影响大。

> 在数据各类样本不均衡的情况下，采用 Micro F1 较为合理。

---

### 一个例子：

以Macro Average 计算为例子

> y_true=[1,2,3]
> y_pred=[1,1,3]

根据P/R的计算规则[1]，

Precision = (预测为1且正确预测的样本数)/(所有预测为1的样本数) = TP/(TP+FP)
Recall = (预测为1且正确预测的样本数)/(所有真实情况为1的样本数) = TP/(TP+FN)
F1 = 2*(Precision*Recall)/(Precision+Recall)
下面计算过程中，若除法过程中，分子分母同时为零，则最终结果也为0.

则Macro Average F1的计算过程如下：

（1）如下，将第1个类别设置为True（1），非第1个类别的设置为False（0），计算其P1,R1

> y_true=[1,0,0]
> y_pred=[1,1,0]

P1 = (预测为1且正确预测的样本数)/(所有预测为1的样本数) = TP/(TP+FP) = 1/(1+1)=0.5
R1 = (预测为1且正确预测的样本数)/(所有真实情况为1的样本数) = TP/(TP+FN)= 1/1 = 1.0
F1_1 = 2*(PrecisionRecall)/(Precision+Recall)=20.5*1.0/(0.5+1.0)=0.6666667



如下，将第2个类别设置为True（1），非第2个类别的设置为False（0），计算其P2,R2

> y_true=[0,1,0]
> y_pred=[0,0,0]

P2 = (预测为1且正确预测的样本数)/(所有预测为1的样本数) = TP/(TP+FP) =0.0
R2 = (预测为1且正确预测的样本数)/(所有真实情况为1的样本数) = TP/(TP+FN)= 0.0
F1_2 = 2*(Precision*Recall)/(Precision+Recall)=0
（3）如下，将第3个类别设置为True（1），非第3个类别的设置为False（0），计算其P3,R3

> y_true=[0,0,1]
> y_pred=[0,0,1]

P3 = (预测为1且正确预测的样本数)/(所有预测为1的样本数) = TP/(TP+FP) = 1/1=1.0
R3 = (预测为1且正确预测的样本数)/(所有真实情况为1的样本数) = TP/(TP+FN)= 1/1 = 1.0
F1_3 = 2*(PrecisionRecall)/(Precision+Recall)=21.0*1.0/(1.0+1.0)=1.0

（4）对P1/P2/P3取平均为P，对R1/R2/R3取平均为R，对F1_1/F1_2/F1_3取平均F1

P=(P1+P2+P3)/3=(0.5+0.0+1.0)/3=0.5
R=(R1+R2+R3)/3=(1.0+0.0+1.0)/3=0.6666666
F1 = (0.6666667+0.0+1.0)/3=0.5556
最后这个取平均后的得到的P值/R值，就是Macro规则下的P值/R值。

对这个3类别模型来说，它的F1就是0.5556。

---

**再看 各个指标**

* 精准率: 预测为真的结果中有多少是真的(**业务含义: 预测出来的结果中，有多少把握说结果是真的**)
* 召回率: 原来为真的有多少被预测为了真的(**业务含义: 将真的样本召回的能力**)

![accuracy](./pic/cm.png)

* 真正率(True Positive Rate , **TPR**)，又称灵敏度：（就是召回率）

![accuracy](./pic/TPR.jpeg)

灵敏度和召回率是一模一样的，只是名字换了而已

* 假负率(False Negative Rate , **FNR**) ：

![accuracy](./pic/FNR.jpeg)

* 假正率(False Positive Rate , **FPR**) ：

![accuracy](./pic/FPR.jpeg)

* 真负率(True Negative Rate , **TNR**)，又称特异度：精确率

![accuracy](./pic/TNR.jpeg)


![accuracy](./pic/cm2.png)

上图，对于不论真实样本是1还是0，分类器预测为1的概率是相等的，也就是说分类器对正例和负列毫无区分能力，和抛硬币没有区别，而我们希望达到的效果是: 对于真实类别为1的样本，分类器预测为1的概率(TPR)要大于分真实类别为0而预测为1的概率(FPR)（富人中最没有钱的人也要比群人中最富有的人的钱多）

需要设置阈值来得到混淆矩阵，不同的阈值会影响得到的TPRate，FPRate，如果阈值取0.5，小于0.5的为0，否则为1，那么我们就得到了与之前一样的混淆矩阵。其他的阈值就不再啰嗦了。依次使用所有预测值作为阈值，得到一系列TPRate，FPRate，描点，求面积，即可得到AUC。

---

AUC 会综合考虑分类器对于整理和负例的分类能力，在样本不平衡的情况下，依然能够对分类器进行合理的评价。

例如在反欺诈场景，设欺诈类样本为正例，正例占比很少（假设0.1%），如果使用准确率评估，把所有的样本预测为负例，便可以获得**99.9%的准确率**。

但是如果使用AUC，把所有样本预测为负例，TPRate和FPRate同时为0（没有Positive），与(0,0) (1,1)连接，得出**AUC仅为0.5**，成功规避了样本不均匀带来的问题。

