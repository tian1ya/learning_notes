```python
1. tf1.x ä¸­é¦–å…ˆè¦åˆ›å»ºè®¡ç®—å›¾
a_ph = tf.placeholder(tf.float32, name="variable_1")
a_pb = tf.placeholder(tf.float32, name="variable_2")

åˆ›å»ºè®¡ç®—å›¾çš„è¿‡ç¨‹å°±ç±»ä¼¼äºé€šè¿‡åˆ›å»ºå…¬å¼ c=a+b çš„è¿‡ç¨‹ï¼Œä»…ä»…æ˜¯è®°å½•äº†å…¬å¼è®¡ç®—æ­¥éª¤
å¹¶æ²¡æœ‰å®é™…è®¡ç®—å…¬å¼çš„æ•°å€¼ç»“æœï¼Œç«¥è°£è¿è¡Œä½¿ç”¨çš„è¾“å‡ºç«¯å­ï¼Œå¹¶èµ‹å€¼ a_ph=1, a_pb=b

c_op = tf.add(a_ph, a_pb, name = "variable_c")

2. è¿è¡Œè®¡ç®—å›¾é˜¶æ®µ, åˆ›å»ºè¿è¡Œç¯å¢ƒ
sess = tf.InteractiveSession()
init = tf.global_variables_initializer()
sess.run(init)

æœ€åæ‰èƒ½æ‹¿åˆ°è®¡ç®—ç»“æœ
c_numpy = sess.run(c_op, feed_dict={a_ph:2, a_pb:4})

åªæ˜¯è®¡ç®—ä¸€ä¸ªç®€å•çš„è®¡ç®—ï¼Œéœ€è¦åšå¾ˆå¤šå…¶ä»–çš„äº‹æƒ…ï¼Œè¿™ç§å…ˆè®¡ç®—å›¾ï¼Œç„¶åè¿è¡Œçš„ç¼–ç¨‹æ–¹å¼æˆä¸ºç¬¦å·å¼ç¼–ç¨‹
"""

"""
ä½¿ç”¨ tf2.xçš„è®¡ç®—
a = tf.constant(2.0)
b = tf.constant(2.0)

c = a + b

è¿™ç§è®¡ç®—æ–¹å¼ç§°ä¸ºæ˜¯å‘½ä»¤å¼ç¼–ç¨‹ï¼Œæˆ–è€…æ˜¯åŠ¨æ€å›¾æ¨¡å¼,è°ƒå¼æ–¹ä¾¿ï¼Œå¼€å‘æ•ˆç‡ä¹Ÿé«˜ï¼Œä½†æ˜¯è¿è¡Œæ•ˆç‡ä¸å¦‚é™æ€å›¾æ¨¡å¼
TensorFlow 2 ä¹Ÿæ”¯æŒé€šè¿‡ tf.function å°†åŠ¨æ€å›¾ä¼˜å…ˆæ¨¡å¼çš„ä»£ç è½¬åŒ–ä¸ºé™æ€å›¾æ¨¡å¼ï¼Œå®ç° å¼€å‘å’Œè¿è¡Œæ•ˆç‡çš„åŒèµ¢
"""

"""
TF ç­‰æ·±åº¦å­¦ä¹ æ¡†æ¶ä¸‰å¤§æ ¸å¿ƒåŠŸèƒ½
1. GPU åŠ é€Ÿè®¡ç®—
2. è‡ªåŠ¨æ¢¯åº¦è®¡ç®—
3. å¸¸ç”¨ç¥ç»ç½‘ç»œæ¥å£ï¼š
    é™¤äº†æä¾›åº•å±‚çš„çŸ©é˜µç›¸ä¹˜ï¼Œç›¸åŠ ç­‰æ•°å­¦è®¡ç®—å‡½æ•°ï¼Œè¿˜å†…å»ºäº†å¸¸ç”¨çš„ç¥ç»ç½‘ç»œè¿ç®—å‡½æ•°ï¼Œå¸¸ç”¨ç½‘ç»œå±‚ï¼Œç½‘ç»œè®­ç»ƒï¼Œæ¨¡å‹ä¿å­˜åŠ è½½ï¼Œç½‘ç»œéƒ¨ç½²ç­‰


åŸºæœ¬çš„ç»´åº¦å˜æ¢æ“ä½œå‡½æ•°åŒ…å«äº†æ”¹å˜è§†å›¾ reshapeã€æ’å…¥æ–°ç»´åº¦ expand_dimsï¼Œåˆ é™¤ç»´ åº¦ squeezeã€äº¤æ¢ç»´åº¦ 
transposeã€å¤åˆ¶æ•°æ® tile ç­‰å‡½æ•°ã€‚

x=tf.range(96) # ç”Ÿæˆå‘é‡
x=tf.reshape(x,[2,4,4,3]) # æ”¹å˜xçš„è§†å›¾ï¼Œè·å¾—4Då¼ é‡ï¼Œå­˜å‚¨å¹¶æœªæ”¹å˜

æ”¹å˜å¼ é‡çš„è§†å›¾ä»…ä»…æ˜¯æ”¹å˜äº†å¼ é‡çš„ç†è§£æ–¹ å¼ï¼Œå¹¶ä¸éœ€è¦æ”¹å˜å¼ é‡çš„å­˜å‚¨é¡ºåºï¼Œè¿™åœ¨ä¸€å®šç¨‹åº¦ä¸Šæ˜¯ä»è®¡ç®—æ•ˆç‡è€ƒè™‘çš„ï¼Œ
å¤§é‡æ•°æ®çš„å†™ å…¥æ“ä½œä¼šæ¶ˆè€—è¾ƒå¤šçš„è®¡ç®—èµ„æºã€‚ç”±äºå­˜å‚¨æ—¶æ•°æ®åªæœ‰å¹³å¦ç»“æ„ï¼Œä¸æ•°æ®çš„é€»è¾‘ç»“æ„æ˜¯åˆ†ç¦»çš„

ä»è¯­æ³•ä¸Šæ¥è¯´ï¼Œè§†å›¾å˜æ¢åªéœ€è¦æ»¡è¶³æ–°è§†å›¾çš„å…ƒç´ æ€»é‡ä¸å­˜å‚¨åŒºåŸŸå¤§å°ç›¸ç­‰å³å¯

å¢åŠ ç»´åº¦ï¼š
    ä»…ä»…æ˜¯æ”¹å˜æ•°æ®çš„ç†è§£æ–¹å¼ï¼Œå› æ­¤å®ƒå…¶å®å¯ä»¥ç†è§£ä¸ºæ”¹ å˜è§†å›¾çš„ä¸€ç§ç‰¹æ®Šæ–¹å¼ã€‚
tf.expand_dims(x, axis)å¯åœ¨æŒ‡å®šçš„ axis è½´å‰å¯ä»¥æ’å…¥ä¸€ä¸ªæ–°çš„ç»´åº¦:
tf.expand_dims çš„ axis ä¸ºæ­£æ—¶ï¼Œè¡¨ç¤ºåœ¨å½“å‰ç»´åº¦ä¹‹å‰æ’å…¥ä¸€ä¸ªæ–°ç»´åº¦;ä¸º è´Ÿæ—¶ï¼Œè¡¨ç¤ºå½“å‰ç»´åº¦ä¹‹åæ’å…¥ä¸€ä¸ªæ–°çš„ç»´åº¦
tf.squeeze(x)ï¼Œé‚£ä¹ˆå®ƒä¼šé»˜è®¤åˆ é™¤æ‰€æœ‰é•¿åº¦ä¸º 1 çš„ç»´åº¦

åˆ é™¤ç»´åº¦ï¼š
    tf.squeeze(x, axis)å‡½æ•°
    
æ”¹å˜è§†å›¾ã€å¢åˆ ç»´åº¦éƒ½ä¸ä¼šå½±å“å¼ é‡çš„å­˜å‚¨ï¼Œæœ‰æ—¶éœ€è¦ç›´æ¥è°ƒæ•´çš„å­˜å‚¨é¡ºåºï¼Œå³äº¤æ¢ ç»´åº¦(Transpose)
é€šè¿‡äº¤æ¢ç»´åº¦æ“ä½œï¼Œæ”¹å˜äº†å¼ é‡çš„å­˜å‚¨é¡ºåºï¼ŒåŒæ—¶ä¹Ÿæ”¹å˜äº†å¼ é‡çš„è§†å›¾ã€‚

tf.transpose(x, perm)å‡½æ•°å®Œæˆç»´åº¦äº¤æ¢æ“ä½œï¼Œå…¶ä¸­å‚æ•° perm è¡¨ç¤ºæ–°ç»´åº¦çš„é¡ºåº Listã€‚
è€ƒè™‘å›¾ç‰‡å¼ é‡ shape ä¸º[2,32,32,3]ï¼Œâ€œå›¾ç‰‡æ•°é‡ã€è¡Œã€åˆ—ã€é€šé“ æ•°â€çš„ç»´åº¦ç´¢å¼•åˆ†åˆ«ä¸º 0ã€1ã€2ã€3ï¼Œ
å¦‚æœéœ€è¦äº¤æ¢ä¸º[ğ‘, ğ‘, h, ]æ ¼å¼ï¼Œåˆ™æ–°ç»´åº¦çš„æ’åºä¸º â€œå›¾ç‰‡æ•°é‡ã€é€šé“æ•°ã€è¡Œã€åˆ—â€ï¼Œå¯¹åº”çš„ç´¢å¼•å·ä¸º[0,3,1,2]
é€šè¿‡ tf.transpose å®Œæˆç»´åº¦äº¤æ¢åï¼Œå¼ é‡çš„å­˜å‚¨é¡ºåºå·²ç»æ”¹å˜ï¼Œè§†å›¾ä¹Ÿ éšä¹‹æ”¹å˜ï¼Œåç»­çš„æ‰€æœ‰æ“ä½œå¿…é¡»åŸºäºæ–°çš„å­˜ç»­é¡ºåºå’Œè§†å›¾è¿›è¡Œ


tf.tile(x, multiples)å‡½æ•°å®Œæˆæ•°æ®åœ¨æŒ‡å®šç»´åº¦ä¸Šçš„å¤åˆ¶æ“ä½œï¼Œmultiples åˆ†åˆ«æŒ‡ å®šäº†æ¯ä¸ªç»´åº¦ä¸Šé¢çš„å¤åˆ¶å€æ•°ï¼Œå¯¹åº”ä½ç½®ä¸º 1 è¡¨æ˜ä¸å¤åˆ¶ï¼Œä¸º 2 è¡¨æ˜æ–°é•¿åº¦ä¸ºåŸæ¥é•¿åº¦çš„ 2 å€ï¼Œå³æ•°æ®å¤åˆ¶ä¸€ä»½ï¼Œä»¥æ­¤ç±»æ¨
b = tf.expand_dims(b, axis=0)
b = tf.tile(b, multiples=[2,1]) 
multiples=[2,1] è¡¨æ˜äº†bæœ‰2ä¸ªç»´åº¦ï¼Œç¬¬ä¸€ä¸ªç»´åº¦ä¹Ÿå°±æ˜¯axis=0ï¼Œå¤åˆ¶ä¸€æ¬¡ï¼Œç¬¬äºŒä¸ªç»´åº¦ï¼Œä¹Ÿå°±æ˜¯axis=1ï¼Œä¸å¤åˆ¶

tf.tile ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„å¼ é‡æ¥ä¿å­˜å¤åˆ¶åçš„å¼ é‡ï¼Œ
ç”±äºå¤åˆ¶æ“ä½œæ¶‰åŠå¤§ é‡æ•°æ®çš„è¯»å†™ IO è¿ç®—ï¼Œè®¡ç®—ä»£ä»·ç›¸å¯¹è¾ƒé«˜

å¹¿æ’­æœºåˆ¶(æˆ–è‡ªåŠ¨æ‰©å±•æœºåˆ¶)
    å®ƒæ˜¯ä¸€ç§è½»é‡çº§çš„å¼ é‡å¤åˆ¶æ‰‹æ®µï¼Œåœ¨é€» è¾‘ä¸Šæ‰©å±•å¼ é‡æ•°æ®çš„å½¢çŠ¶ï¼Œä½†æ˜¯åªä¼šåœ¨éœ€è¦æ—¶æ‰ä¼šæ‰§è¡Œå®é™…å­˜å‚¨å¤åˆ¶æ“ä½œã€‚
å¯¹äºå¤§éƒ¨åˆ†åœº æ™¯ï¼ŒBroadcasting æœºåˆ¶éƒ½èƒ½é€šè¿‡ä¼˜åŒ–æ‰‹æ®µé¿å…å®é™…å¤åˆ¶æ•°æ®è€Œå®Œæˆé€»è¾‘è¿ç®—ï¼Œ
ä»è€Œç›¸å¯¹äº tf.tile å‡½æ•°ï¼Œå‡å°‘äº†å¤§é‡è®¡ç®—ä»£ä»·ã€‚

å¯¹äºç”¨æˆ·æ¥è¯´ï¼ŒBroadcasting å’Œ tf.tile å¤ åˆ¶çš„æœ€ç»ˆæ•ˆæœæ˜¯ä¸€æ ·çš„ï¼Œæ“ä½œå¯¹ç”¨æˆ·é€æ˜ï¼Œ
ä½†æ˜¯ Broadcasting æœºåˆ¶èŠ‚çœäº†å¤§é‡è®¡ç®—èµ„æºï¼Œ å»ºè®®åœ¨è¿ç®—è¿‡ç¨‹ä¸­å°½å¯èƒ½åœ°åˆ©ç”¨ Broadcasting æœºåˆ¶æé«˜è®¡ç®—æ•ˆç‡ã€‚

æ“ä½œç¬¦+åœ¨é‡åˆ° shape ä¸ä¸€è‡´çš„ 2 ä¸ªå¼ é‡æ—¶ï¼Œä¼šè‡ªåŠ¨è€ƒè™‘å°† 2 ä¸ªå¼ é‡è‡ªåŠ¨æ‰©å±•åˆ° ä¸€è‡´çš„ shapeï¼Œç„¶åå†è°ƒç”¨ 
tf.add å®Œæˆå¼ é‡ç›¸åŠ è¿ç®—

tf.add, tf.subtract, tf.multiply, tf.divide
TensorFlow å·²ç»é‡è½½äº†+ã€ âˆ’ ã€ âˆ— ã€/è¿ç®—ç¬¦ï¼Œä¸€èˆ¬æ¨èç›´æ¥ä½¿ç”¨è¿ç®—ç¬¦æ¥å®Œæˆ åŠ ã€å‡ã€ä¹˜ã€é™¤è¿ç®—ã€‚
æ•´é™¤å’Œä½™é™¤ä¹Ÿæ˜¯å¸¸è§çš„è¿ç®—ä¹‹ä¸€ï¼Œåˆ†åˆ«é€šè¿‡//å’Œ%è¿ç®—ç¬¦å®ç°

çŸ©é˜µç›¸ä¹˜ï¼š
    é€šè¿‡@è¿ç®—ç¬¦å¯ä»¥æ–¹ä¾¿çš„å®ç°çŸ©é˜µç›¸ä¹˜ï¼Œ è¿˜å¯ä»¥é€šè¿‡ tf.matmul(a, b)å‡½æ•°å®ç°
    
çŸ©é˜µæ‰¹ç›¸ä¹˜
    ä¹Ÿå°±æ˜¯å¼ é‡ğ‘¨å’Œğ‘©çš„ç»´åº¦æ•°å¯ä»¥å¤§äº 2ï¼Œå½“å¼ é‡ğ‘¨å’Œğ‘©ç»´åº¦æ•°å¤§ äº 2 æ—¶ï¼ŒTensorFlow ä¼šé€‰æ‹©ğ‘¨å’Œğ‘©çš„æœ€åä¸¤ä¸ªç»´åº¦è¿›è¡ŒçŸ©é˜µç›¸ä¹˜ï¼Œå‰é¢æ‰€æœ‰çš„ç»´åº¦éƒ½è§†ä½œ Batch ç»´åº¦ã€‚
    æ ¹æ®çŸ©é˜µç›¸ä¹˜çš„å®šä¹‰ï¼Œğ‘¨å’Œğ‘©èƒ½å¤ŸçŸ©é˜µç›¸ä¹˜çš„æ¡ä»¶æ˜¯ï¼Œğ‘¨çš„å€’æ•°ç¬¬ä¸€ä¸ªç»´åº¦é•¿åº¦(åˆ—)å’Œğ‘© çš„å€’æ•°ç¬¬äºŒä¸ªç»´åº¦é•¿åº¦(è¡Œ)å¿…é¡»ç›¸ç­‰ã€‚
    æ¯”å¦‚å¼ é‡ a shape:[4,3,28,32]å¯ä»¥ä¸å¼ é‡ b shape:[4,3,32,2]è¿›è¡ŒçŸ©é˜µç›¸ä¹˜
    
    a = tf.random.normal([4,3,28,32]) 
    b = tf.random.normal([4,3,32,2])
    a@b # shape=(4, 3, 28, 2)
    
    çŸ©é˜µç›¸ä¹˜å‡½æ•°åŒæ ·æ”¯æŒè‡ªåŠ¨ Broadcasting æœºåˆ¶
        a = tf.random.normal([4,28,32])
        b = tf.random.normal([32,16])
        tf.matmul(a,b) # shape=(4, 28, 16)
    ä¸Šè¿°è¿ç®—è‡ªåŠ¨å°†å˜é‡ b æ‰©å±•ä¸ºå…¬å…± shape:[4,32,16]ï¼Œå†ä¸å˜é‡ a è¿›è¡Œæ‰¹é‡å½¢å¼åœ°çŸ©é˜µç›¸ä¹˜ï¼Œ
    å¾—åˆ°ç»“æœçš„ shape ä¸º[4,28,16]

åˆå¹¶ä¸åˆ†å‰²
    å¯ä»¥ä½¿ç”¨æ‹¼æ¥(Concatenate)å’Œå †å (Stack)æ“ä½œå®ç°ï¼Œæ‹¼æ¥æ“ä½œå¹¶ä¸ä¼šäº§ç”Ÿæ–° çš„ç»´åº¦ï¼Œä»…åœ¨ç°æœ‰çš„ç»´åº¦ä¸Šåˆå¹¶ï¼Œ
    è€Œå †å ä¼šåˆ›å»ºæ–°ç»´åº¦
    tf.concat(tensors, axis)     
    tensors: ä¿å­˜äº†æ‰€æœ‰éœ€è¦åˆå¹¶çš„å¼ é‡ List
    axis å‚æ•°æŒ‡å®šéœ€è¦åˆå¹¶çš„ç»´åº¦ç´¢å¼•
    a = tf.random.normal([4,35,8])
    b = tf.random.normal([6,35,8]) 
    tf.concat([a,b],axis=0) # shape=(10, 35, 8)
    
    a = tf.random.normal([10,35,4])
    b = tf.random.normal([10,35,4]) 
    tf.concat([a,b],axis=2) # shape=(10, 35, 8)
    æ‹¼æ¥åˆå¹¶æ“ä½œå¯ä»¥åœ¨ä»»æ„çš„ç»´åº¦ä¸Šè¿›è¡Œï¼Œå”¯ä¸€çš„çº¦æŸæ˜¯éåˆå¹¶ç»´åº¦çš„ é•¿åº¦å¿…é¡»ä¸€è‡´

åˆ†å‰²
    åˆå¹¶æ“ä½œçš„é€†è¿‡ç¨‹å°±æ˜¯åˆ†å‰²ï¼Œå°†ä¸€ä¸ªå¼ é‡åˆ†æ‹†ä¸ºå¤šä¸ªå¼ é‡
     tf.split(x, num_or_size_splits, axis)
        xå‚æ•°:å¾…åˆ†å‰²å¼ é‡ã€‚
        num_or_size_splitså‚æ•°:åˆ‡å‰²æ–¹æ¡ˆã€‚å½“num_or_size_splitsä¸ºå•ä¸ªæ•°å€¼æ—¶ï¼Œ
            å¦‚10ï¼Œè¡¨ ç¤ºç­‰é•¿åˆ‡å‰²ä¸º 10 ä»½;å½“ num_or_size_splits ä¸º List æ—¶ï¼Œ
            List çš„æ¯ä¸ªå…ƒç´ è¡¨ç¤ºæ¯ä»½çš„é•¿ åº¦ï¼Œå¦‚[2,4,2,2]è¡¨ç¤ºåˆ‡å‰²ä¸º 4 ä»½ï¼Œæ¯ä»½çš„é•¿åº¦ä¾æ¬¡æ˜¯ 2ã€4ã€2ã€2
        axiså‚æ•°:æŒ‡å®šåˆ†å‰²çš„ç»´åº¦ç´¢å¼•å·ã€‚
        
æœ€å€¼ã€å‡å€¼ã€å’Œ
    tf.reduce_maxã€tf.reduce_minã€tf.reduce_meanã€tf.reduce_sum å‡½æ•°å¯ä»¥æ±‚è§£å¼ é‡
    åœ¨æŸä¸ªç»´åº¦ä¸Šçš„æœ€å¤§ã€æœ€å°ã€å‡å€¼ã€å’Œï¼Œä¹Ÿå¯ä»¥æ±‚å…¨å±€æœ€å¤§ã€æœ€å°ã€å‡å€¼ã€å’Œä¿¡æ¯ã€‚  
    tf.reduce_max(x,axis=0), ç¬¬ä¸€ä¸ªç»´åº¦çš„æœ€å¤§å€¼ï¼Œå½“ä¸æŒ‡å®š axisçš„æ—¶å€™å°±æ˜¯ å…¨å±€çš„æœ€å¤§å€¼
    
å¡«å……
    tf.pad(x, paddings)
        å‚æ•° paddings æ˜¯åŒ…å«äº†å¤šä¸ª
        [Left Padding, Right Padding]çš„åµŒå¥—æ–¹æ¡ˆ Listï¼Œå¦‚[[0,0], [2,1], [1,2]]è¡¨ç¤ºç¬¬ä¸€ä¸ªç»´åº¦ä¸å¡«å……ï¼Œ
        ç¬¬äºŒä¸ªç»´åº¦å·¦è¾¹(èµ·å§‹å¤„)å¡«å……ä¸¤ä¸ªå•å…ƒï¼Œå³è¾¹(ç»“æŸå¤„)å¡«å……ä¸€ä¸ªå•å…ƒï¼Œç¬¬ä¸‰ä¸ªç»´åº¦å·¦è¾¹ å¡«å……ä¸€ä¸ªå•å…ƒï¼Œå³è¾¹å¡«å……ä¸¤ä¸ªå•å…ƒ
    
    b = tf.constant([7,8,1,6])
    b = tf.pad(b, [[0,2]]) # å¥å­æœ«å°¾å¡«å…… 2 ä¸ª 0ï¼Œarray([7, 8, 1, 6, 0, 0])
    
    x = tf.random.normal([4,28,28,1])
    tf.pad(x,[[0,0],[2,2],[2,2],[0,0]]) # å¡«å……æ•ˆæœæŸ¥çœ‹æˆªå›¾
    
å¤åˆ¶
    tf.tile å‡½æ•°å¯ä»¥åœ¨ä»»æ„ç»´åº¦å°†æ•°æ®é‡å¤å¤åˆ¶å¤šä»½
    x = tf.random.normal([4,32,32,3])
    tf.tile(x,[2,3,3,1]) # æ•°æ®å¤åˆ¶, ç»“æœ shape=(8, 96, 96, 3)
  
é«˜çº§æ“ä½œ
    tf.gather  
        å®ç°æ ¹æ®ç´¢å¼•å·æ”¶é›†æ•°æ®çš„ç›®çš„
        4ä¸ªç­çº§ï¼Œæ¯ä¸ªç­çº§ 35 ä¸ªå­¦ç”Ÿï¼Œ8 é—¨ç§‘ç›®
        x = tf.random.uniform([4,35,8],maxval=100,dtype=tf.int32) # æˆç»©å†Œå¼ é‡
        ç°åœ¨éœ€è¦æ”¶é›†ç¬¬ 1~2 ä¸ªç­çº§çš„æˆç»©å†Œ
        tf.gather(x,[0,1],axis=0) # åœ¨ç­çº§ç»´åº¦æ”¶é›†ç¬¬ 1~2 å·ç­çº§æˆç»©å†Œ
        
        å¦‚æœå¸Œæœ›æŠ½æŸ¥ç¬¬[2,3]ç­çº§çš„ç¬¬[3,4,6,27]å·åŒå­¦çš„ç§‘ç›® æˆç»©ï¼Œåˆ™å¯ä»¥é€šè¿‡ç»„åˆå¤šä¸ª tf.gather å®ç°
        students=tf.gather(x,[1,2],axis=0) # æ”¶é›†ç¬¬ 2,3 å·ç­çº§ shape=(2, 35, 8)
        tf.gather(students,[2,3,5,26],axis=1) # æ”¶é›†ç¬¬ 3,4,6,27 å·åŒå­¦ shape=(2, 4, 8)
        
        æˆ‘ä»¬ç»§ç»­é—®é¢˜è¿›ä¸€æ­¥å¤æ‚åŒ–ã€‚è¿™æ¬¡æˆ‘ä»¬å¸Œæœ›æŠ½æŸ¥ç¬¬ 2 ä¸ªç­çº§çš„ç¬¬ 2 ä¸ªåŒå­¦çš„æ‰€æœ‰ç§‘ ç›®ï¼Œ
        ç¬¬ 3 ä¸ªç­çº§çš„ç¬¬ 3 ä¸ªåŒå­¦çš„æ‰€æœ‰ç§‘ç›®ï¼Œç¬¬ 4 ä¸ªç­çº§çš„ç¬¬ 4 ä¸ªåŒå­¦çš„æ‰€æœ‰ç§‘ç›®
        
    tf.gather_nd
        æŒ‡å®šæ¯æ¬¡é‡‡æ ·ç‚¹çš„å¤šç»´åæ ‡æ¥å®ç°é‡‡æ ·å¤šä¸ªç‚¹çš„ç›® çš„
        tf.gather_nd(x,[[1,1],[2,2],[3,3]]) # shape=(3, 8)
    
    tf.boolean_mask
        å¯ä»¥é€šè¿‡ç»™å®šæ©ç (Mask)çš„æ–¹å¼è¿›è¡Œé‡‡æ ·
        å³é‡‡æ ·ç¬¬ 1 å’Œç¬¬ 4 ä¸ªç­çº§çš„æ•°æ®ï¼š mask = [True, False, False, True]
        tf.boolean_mask(x, mask, axis)å¯ä»¥åœ¨ axis è½´ä¸Šæ ¹æ® mask æ–¹æ¡ˆè¿›è¡Œé‡‡æ ·
        tf.boolean_mask(x,mask=[True, False,False,True],axis=0) shape=(2, 35, 8)
        æ³¨æ„æ©ç çš„é•¿åº¦å¿…é¡»ä¸å¯¹åº”ç»´åº¦çš„é•¿åº¦ä¸€è‡´ï¼Œå¦‚åœ¨ç­çº§ç»´åº¦ä¸Šé‡‡æ ·ï¼Œåˆ™å¿…é¡»å¯¹è¿™ 4 ä¸ªç­çº§ æ˜¯å¦é‡‡æ ·çš„æ©ç å…¨éƒ¨æŒ‡å®šï¼Œæ©ç é•¿åº¦ä¸º 4
        
        è€ƒè™‘ä¸ tf.gather_nd ç±»ä¼¼æ–¹å¼çš„å¤šç»´æ©ç é‡‡æ ·æ–¹å¼
    
        x: shape=(4, 8)
        tf.boolean_mask(x,[[True,True,False],[False,True,True]])
           
    tf.where
        tf.where(cond, a, b) cond==True ? a : b
        a = tf.ones([3,3]) # æ„é€  a ä¸ºå…¨ 1 çŸ©é˜µ
        b = tf.zeros([3,3]) # æ„é€  b ä¸ºå…¨ 0 çŸ©é˜µ
        cond = tf.constant([[True,False,False],[False,True,False],[True,True,False]])
        tf.where(cond,a,b) # æ ¹æ®æ¡ä»¶ä» a,b ä¸­é‡‡æ ·
        åˆ†åˆ«æŒ‰ç…§condä¸­æ¯ä¸€ä¸ªå…ƒç´ çš„Trueæˆ–è€…False ç„¶åè¿”å›aä¸­æˆ–è€…bä¸­çš„å…ƒç´ 
        tf.where(cond)å½¢å¼æ¥è·å¾—è¿™äº›å…ƒç´ çš„ç´¢å¼•åæ ‡
        
    ä¸€ä¸ªç»¼åˆå®ä¾‹
        éœ€è¦æå–å¼ é‡ä¸­æ‰€æœ‰æ­£æ•°çš„æ•°æ®å’Œç´¢å¼•
        x = tf.random.normal([3,3]) # æ„é€  a
        mask=x>0
        indices=tf.where(mask) # æå–æ‰€æœ‰å¤§äº 0 çš„å…ƒç´ ç´¢å¼•
        tf.gather_nd(x,indices) # æå–æ­£æ•°çš„å…ƒç´ å€¼


tf.data.Datasetï¼š 
    æ•°æ®é›†å¯¹è±¡ï¼Œæ–¹ä¾¿å®ç°å¤šçº¿ç¨‹(Multi-threading)ã€é¢„ å¤„ç†(Preprocessing)ã€éšæœºæ‰“æ•£(Shuffle)å’Œæ‰¹è®­ç»ƒ(Training on Batch)ç­‰å¸¸ç”¨æ•°æ®é›†çš„åŠŸèƒ½ã€‚
    æ•°æ®åŠ è½½è¿›å…¥å†…å­˜åï¼Œéœ€è¦è½¬æ¢æˆ Dataset å¯¹è±¡ï¼Œæ‰èƒ½åˆ©ç”¨ TensorFlow æä¾›çš„å„ç§ä¾¿ æ·åŠŸèƒ½
    Dataset.from_tensor_slices å¯ä»¥å°†è®­ç»ƒéƒ¨åˆ†çš„æ•°æ®å›¾ç‰‡ x å’Œæ ‡ç­¾ y éƒ½è½¬æ¢æˆ Dataset å¯¹è±¡
        train_db = tf.data.Dataset.from_tensor_slices((x, y)) # æ„å»º Dataset å¯¹è±¡
   
    Dataset.shuffle(buffer_size)å·¥å…·å¯ä»¥è®¾ç½® Dataset å¯¹è±¡éšæœºæ‰“æ•£æ•°æ®ä¹‹é—´çš„é¡ºåº
        train_db = train_db.shuffle(10000) # éšæœºæ‰“æ•£æ ·æœ¬ï¼Œä¸ä¼šæ‰“ä¹±æ ·æœ¬ä¸æ ‡ç­¾æ˜ å°„å…³ç³»
        buffer_size å‚æ•°æŒ‡å®šç¼“å†²æ± çš„å¤§å°ï¼Œå°†æ‰“æ•£çš„æ•°æ®æ”¾å…¥åˆ°è¿™ä¸ªç¼“å†²æ± ä¸­ï¼Œç„¶åä»è¿™ä¸ªæ± å­ä¸­å–æ•°æ®
    
    æ‰¹è®­ç»ƒ
        ä¸€æ¬¡èƒ½å¤Ÿä» Dataset ä¸­äº§ç”Ÿ Batch Size æ•°é‡çš„æ ·æœ¬ï¼Œéœ€è¦è®¾ç½® Dataset ä¸ºæ‰¹è®­ç»ƒæ–¹å¼
        train_db = train_db.batch(128)
        
    é¢„å¤„ç†
        
    å¾ªç¯è®­ç»ƒ
        for step, (x,y) in enumerate(train_db): # è¿­ä»£æ•°æ®é›†å¯¹è±¡ï¼Œå¸¦ step å‚æ•°
        æˆ–è€…ï¼šfor x,y in train_db: # è¿­ä»£æ•°æ®é›†å¯¹è±¡
        æ¯æ¬¡è¿”å›çš„ x å’Œ y å¯¹è±¡å³ä¸ºæ‰¹é‡æ ·æœ¬å’Œæ ‡ç­¾ã€‚å½“å¯¹ train_db çš„æ‰€æœ‰æ ·æœ¬å®Œ æˆä¸€æ¬¡è¿­ä»£åï¼Œfor å¾ªç¯ç»ˆæ­¢é€€å‡º
        
        è¿™é‡Œä½¿ç”¨çš„for è¿­ä»£ï¼Œè¿­ä»£å®Œæ•°æ®åé€€å‡ºfor(æ¯æ¬¡for loop è¿”å›batch_size ä¸ªæ•°æ®)
        ä¹Ÿå¯ä»¥è®¾ç½®ä¸€ä¸ªå…¨é‡æ•°æ®ä¸­è®¾ç½®å¤šå°‘ä¸ªfor loop
        train_db = train_db.repeat(20) # æ•°æ®é›†è¿­ä»£ 20 éæ‰ç»ˆæ­¢
        
        
---
```



`Keras æ¨¡å—åŒ–çš„å…¨è¿æ¥å±‚`

```python
from tensorflow.keras import layers # å¯¼å…¥å±‚æ¨¡å—
fc = layers.Dense(512, activation=tf.nn.relu)
h1 = fc(x) # é€šè¿‡ fc ç±»å®ä¾‹å®Œæˆä¸€æ¬¡å…¨è¿æ¥å±‚çš„è®¡ç®—ï¼Œè¿”å›è¾“å‡ºå¼ é‡ è°ƒç”¨ç±»çš„__call__æ–¹æ³•å³å¯
fc.kernel # è·å– Dense ç±»çš„æƒå€¼çŸ©é˜µ
fc.bias # è·å– Dense ç±»çš„åç½®å‘é‡

# fc.trainable_variables è¿”å›å¾…ä¼˜åŒ–çš„å‚æ•°åˆ—è¡¨
# fc.variables è¿”å›æ‰€æœ‰å‚æ•°åˆ—è¡¨
```

`å¼ é‡çš„æ–¹å¼å®ç°å¤šå±‚å‡çº§ç½‘ç»œ`

```python
# éšè—å±‚1å¼ é‡
w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))
b1 = tf.Variable(tf.zeros([256]))
# éšè—å±‚2å¼ é‡
w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1)) b2 = tf.Variable(tf.zeros([128]))
# éšè—å±‚3å¼ é‡
w3 = tf.Variable(tf.random.truncated_normal([128, 64], stddev=0.1)) b3 = tf.Variable(tf.zeros([64]))
# è¾“å‡ºå±‚å¼ é‡
w4 = tf.Variable(tf.random.truncated_normal([64, 10], stddev=0.1))
b4 = tf.Variable(tf.zeros([10]))
```

`è®¡ç®—æ—¶ï¼Œåªéœ€è¦æŒ‰ç…§ç½‘ç»œå±‚çš„é¡ºåºï¼Œå°†ä¸Šä¸€å±‚çš„è¾“å‡ºä½œä¸ºå½“å‰å±‚çš„è¾“å…¥å³å¯ï¼Œé‡å¤ ç›´è‡³æœ€åä¸€å±‚ï¼Œå¹¶å°†è¾“å‡ºå±‚çš„è¾“å‡ºä½œä¸ºç½‘ç»œçš„è¾“å‡º`

```python
with tf.GradientTape() as tape: # æ¢¯åº¦è®°å½•å™¨
  # x: [b, 28*28]
	# éšè—å±‚ 1 å‰å‘è®¡ç®—ï¼Œ[b, 28*28] => [b, 256]
	h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256]) h1 = tf.nn.relu(h1)
	# éšè—å±‚ 2 å‰å‘è®¡ç®—ï¼Œ[b, 256] => [b, 128]
	h2 = h1@w2 + b2
	h2 = tf.nn.relu(h2)
	# éšè—å±‚ 3 å‰å‘è®¡ç®—ï¼Œ[b, 128] => [b, 64] h3 = h2@w3 + b3
	h3 = tf.nn.relu(h3)
	# è¾“å‡ºå±‚å‰å‘è®¡ç®—ï¼Œ[b, 64] => [b, 10] h4 = h3@w4 + b4
```

`åœ¨ä½¿ç”¨Tensorflow è‡ªåŠ¨ æ±‚å¯¼åŠŸèƒ½è®¡ç®—æ¢¯åº¦æ—¶ï¼Œéœ€è¦å°†å‰å‘è®¡ç®—è¿‡ç¨‹æ”¾ç½®åœ¨  tf.GradientTape() ç¯å¢ƒä¸­ï¼Œä»è€Œåˆ©ç”¨GradientTape å¯¹è±¡çš„ gradient æ–¹æ³•è‡ªåŠ¨æ±‚è§£å‚æ•°çš„æ¢¯åº¦`

`ä½¿ç”¨Keras åºåˆ—å±‚å®ç°(åªèƒ½ å®ç°ç®€å•çš„å¸¸è§„çš„å…¨è¿æ¥å±‚)`

```python
from tensorflow.keras import layers,Sequential
fc1 = layers.Dense(256, activation=tf.nn.relu) # éšè—å±‚1 
fc2 = layers.Dense(128, activation=tf.nn.relu) # éšè—å±‚2
fc3 = layers.Dense(64, activation=tf.nn.relu) # éšè—å±‚3 
fc4 = layers.Dense(10, activation=None) # è¾“å‡ºå±‚
x = tf.random.normal([4,28*28]) h1 = fc1(x) # é€šè¿‡éšè—å±‚ 1 å¾—åˆ°è¾“å‡º 
h2 = fc2(h1) # é€šè¿‡éšè—å±‚ 2 å¾—åˆ°è¾“å‡º
h3 = fc3(h2) # é€šè¿‡éšè—å±‚ 3 å¾—åˆ°è¾“å‡º 
h4 = fc4(h3) # é€šè¿‡è¾“å‡ºå±‚å¾—åˆ°ç½‘ç»œè¾“å‡º
```

`æˆ–è€…`

```python
model = Sequential([
		layers.Dense(256, activation=tf.nn.relu) , # åˆ›å»ºéšè—å±‚ 1
  	layers.Dense(128, activation=tf.nn.relu) , # åˆ›å»ºéšè—å±‚ 2 
  	layers.Dense(64, activation=tf.nn.relu) , # åˆ›å»ºéšè—å±‚ 3
		layers.Dense(10, activation=None) , # åˆ›å»ºè¾“å‡ºå±‚ 
])

out = model(x) # å‰å‘è®¡ç®—å¾—åˆ°è¾“å‡º
```

`å®ç°ä¸ºä¸€ä¸ªè‡ªå®šä¹‰ç½‘ç»œç±»ï¼Œåªéœ€è¦åœ¨åˆå§‹åŒ–å‡½æ•°ä¸­åˆ›å»ºå„ä¸ªå­ç½‘ç»œå±‚ï¼Œå¹¶ åœ¨å‰å‘è®¡ç®—å‡½æ•° call ä¸­å®ç°è‡ªå®šä¹‰ç½‘ç»œç±»çš„è®¡ç®—é€»è¾‘å³å¯ã€‚è‡ªå®šä¹‰ç½‘ç»œç±»ç»§æ‰¿è‡ª keras.Model åŸºç±»ï¼Œè¿™ä¹Ÿæ˜¯è‡ªå®šä¹‰ç½‘ç»œç±»çš„æ ‡å‡†å†™æ³•ï¼Œä»¥æ–¹ä¾¿åœ°åˆ©ç”¨ keras.Model åŸºç±»æä¾› çš„ trainable_variablesã€save_weights ç­‰å„ç§ä¾¿æ·åŠŸèƒ½`

```python
class Network(keras.Model): 
  # å›å½’ç½‘ç»œæ¨¡å‹
	def __init__(self):
			super(Network, self).__init__()
			# åˆ›å»º3ä¸ªå…¨è¿æ¥å±‚
			self.fc1 = layers.Dense(64, activation='relu')
  	  self.fc2 = layers.Dense(64, activation='relu')
    	self.fc3 = layers.Dense(1)
    
  def call(self, inputs, training=None, mask=None):
    	x = self.fc1(inputs)
			x = self.fc2(x)
			x = self.fc3(x)
			return x
    
    
# æ„å»ºDataset å¯¹è±¡
train_db = tf.data.Dataset.from_tensor_slices((normed_train_data.values,
train_labels.values)) # æ„å»º Dataset å¯¹è±¡
train_db = train_db.shuffle(100).batch(32) # éšæœºæ‰“æ•£ï¼Œæ‰¹é‡åŒ–

model = Network() # åˆ›å»ºç½‘ç»œç±»å®ä¾‹
# é€šè¿‡ build å‡½æ•°å®Œæˆå†…éƒ¨å¼ é‡çš„åˆ›å»ºï¼Œå…¶ä¸­ 4 ä¸ºä»»æ„è®¾ç½®çš„ batch æ•°é‡ï¼Œ9 ä¸ºè¾“å…¥ç‰¹å¾é•¿åº¦
model.build(input_shape=(4, 9))
model.summary() # æ‰“å°ç½‘ç»œä¿¡æ¯
optimizer = tf.keras.optimizers.RMSprop(0.001) # åˆ›å»ºä¼˜åŒ–å™¨ï¼ŒæŒ‡å®šå­¦ä¹ ç‡

for epoch in range(200): # 200ä¸ªEpoch
	for step, (x,y) in enumerate(train_db): # éå†ä¸€æ¬¡è®­ç»ƒé›†
			# æ¢¯åº¦è®°å½•å™¨ï¼Œè®­ç»ƒæ—¶éœ€è¦ä½¿ç”¨å®ƒ
			with tf.GradientTape() as tape:
			out = model(x) # é€šè¿‡ç½‘ç»œè·å¾—è¾“å‡º
			loss = tf.reduce_mean(losses.MSE(y, out)) # è®¡ç®— 
      MSE mae_loss = tf.reduce_mean(losses.MAE(y, out)) # è®¡ç®— MAE
			if step % 10 == 0: 
        # é—´éš”æ€§åœ°æ‰“å°è®­ç»ƒè¯¯å·® 
        print(epoch, step, float(loss))
			
      # è®¡ç®—æ¢¯åº¦ï¼Œå¹¶æ›´æ–°
			grads = tape.gradient(loss, model.trainable_variables)
			optimizer.apply_gradients(zip(grads, model.trainable_variables))
```



`è‡ªåŠ¨æ±‚å¯¼çš„ä½¿ç”¨` **è¿™é‡Œæ³¨æ„ é variable å˜é‡è‡ªåŠ¨æ±‚å¯¼çš„ä½¿ç”¨**

```python
import tensorflow as tf
# æ„å»ºå¾…ä¼˜åŒ–å˜é‡
x = tf.constant(1.)
w1 = tf.constant(2.)
b1 = tf.constant(1.)
w2 = tf.constant(2.)
b2 = tf.constant(1.)
# æ„å»ºæ¢¯åº¦è®°å½•å™¨
with tf.GradientTape(persistent=True) as tape:
  # é tf.Variable ç±»å‹çš„å¼ é‡éœ€è¦äººä¸ºè®¾ç½®è®°å½•æ¢¯åº¦ä¿¡æ¯ 
  tape.watch([w1, b1, w2, b2])
  # æ„å»º2å±‚çº¿æ€§ç½‘ç»œ 
  y1 = x * w1 + b1 
  y2 = y1 * w2 + b2
  
  
dy2_dy1 = tape.gradient(y2, [y1])[0] 
dy1_dw1 = tape.gradient(y1, [w1])[0] 
dy2_dw1 = tape.gradient(y2, [w1])[0]

# éªŒè¯é“¾å¼æ³•åˆ™ï¼Œ2 ä¸ªè¾“å‡ºåº”ç›¸ç­‰
print(dy2_dy1 * dy1_dw1)
print(dy2_dw1)
```

---

**Keras é«˜å±‚æ¥å£**

`keras æ˜¯ä¸€ä¸ªé«˜åº¦æ¨¡å—åŒ–å’Œæ˜“æ‰©å±•çš„é«˜å±‚ç¥ç»ç½‘ç»œæ¥å£ï¼Œä½¿å¾—ç”¨æˆ·å¯ä»¥ä¸éœ€è¦è¿‡å¤šçš„ä¸“ä¸šçŸ¥è¯†å°±å¯ä»¥ç®€æ´ï¼Œå¿«é€Ÿçš„å®Œæˆæ¨¡å‹çš„æ­å»ºå’Œè®­ç»ƒï¼Œåœ¨ä¸€å¼€å§‹çš„è®¾è®¡ ä¸­ keras è¢«è®¾è®¡ä¸ºå‰ç«¯å’Œåç«¯éƒ¨åˆ†ï¼Œå‰æ®µå°±æ˜¯è¿™é‡Œä»–æåˆ°çš„é«˜å±‚ç¥ç»ç½‘ç»œæ¥å£ï¼Œè€Œè€Œåç«¯å°±æ˜¯ä»¥TFã€CNTK ç­‰çš„åº•å±‚å®ç°ï¼Œåœ¨TF2.x ä¹‹åKeras å’Œ TF äºŒè€…èåˆï¼Œå¹¶ä¸”Keras ä½œä¸ºTFé«˜å±‚æ¨¡å—çš„å”¯ä¸€APIï¼Œå–ä»£TFåŸæ¥çš„tf.layers`



`2 ä¸ªç‰¹æ®Šçš„ç±» keras.Model å’Œ keras.layer.Layer  Layer ç±»æ˜¯ç½‘ç»œå±‚çš„æ¯ç±»ï¼Œå®šä¹‰äº†ç½‘ç»œå±‚çš„ä¸€äº›å¸¸è§åŠŸèƒ½ï¼Œå¦‚æ·»åŠ æƒå€¼ã€ç®¡ç†æƒå€¼åˆ—è¡¨ç­‰ã€‚ Model ç±»æ˜¯ç½‘ç»œçš„æ¯ç±»ï¼Œé™¤äº†å…·æœ‰ Layer ç±»çš„åŠŸèƒ½ï¼Œè¿˜æ·»åŠ äº†ä¿å­˜æ¨¡å‹ã€åŠ è½½æ¨¡å‹ã€è®­ç»ƒ ä¸æµ‹è¯•æ¨¡å‹ç­‰ä¾¿æ·åŠŸèƒ½ã€‚Sequential ä¹Ÿæ˜¯ Model çš„å­ç±»ï¼Œå› æ­¤å…·æœ‰ Model ç±»çš„æ‰€æœ‰åŠŸèƒ½`

**è‡ªå®šä¹‰Layer**

`è‡³å°‘å®ç° __init__ æ–¹æ³•å’Œ call æ–¹æ³•`

```python
class MyDense(layers.Layer): # è‡ªå®šä¹‰ç½‘ç»œå±‚
		def __init__(self, inp_dim, outp_dim):
				super(MyDense, self).__init__()
				# åˆ›å»ºæƒå€¼å¼ é‡å¹¶æ·»åŠ åˆ°ç±»ç®¡ç†åˆ—è¡¨ä¸­ï¼Œè®¾ç½®ä¸ºéœ€è¦ä¼˜åŒ–
        # trainable=True å‘½åä¸ºw çš„æƒé‡ä¼šè¢« trainable_variables ç®¡ç†
        # å¦‚æœ trainable=False åˆ™ä¸ä¼š
				self.kernel = self.add_variable('w', [inp_dim, outp_dim], trainable=True)

    # å®ç°è‡ªå®šä¹‰ç±»çš„å‰å‘è®¡ç®—é€»è¾‘    
		def call(self, inputs, training=None):
        # training è®¾ç½®layer çŠ¶æ€True é‚£ä¹ˆæƒé‡ä¼šè¢«è®­ç»ƒ 
        # training ä¸º False æ—¶æ‰§è¡Œæµ‹è¯•æ¨¡å¼ï¼Œé»˜è®¤å‚æ•°ä¸º Noneï¼Œå³æµ‹è¯•æ¨¡å¼
      	out = inputs @ self.kernel
        out = tf.nn.relu(out)
        return out
```

**è‡ªå®šä¹‰ç½‘ç»œç±»**

```python
class MyModel(keras.Model):
  	# è‡ªå®šä¹‰ç½‘ç»œç±»ï¼Œç»§æ‰¿è‡ª Model åŸºç±»
    def __init__(self):
				super(MyModel, self).__init__() # å®Œæˆç½‘ç»œå†…éœ€è¦çš„ç½‘ç»œå±‚çš„åˆ›å»ºå·¥ä½œ 
        self.fc1 = MyDense(28*28, 256) 
        self.fc2 = MyDense(256, 128) 
        self.fc3 = MyDense(128, 64)
				self.fc4 = MyDense(64, 32)
				self.fc5 = MyDense(32, 10)
        
    # ç½‘ç»œçš„å‰å‘è¿ç®—é€»è¾‘
    def call(self, inputs, training=None):
      	x = self.fc1(inputs)
				x = self.fc2(x)
				x = self.fc3(x)
				x = self.fc4(x)
				x = self.fc5(x)
				return x
```

`å¯¹äºå¸¸ç”¨çš„ç½‘ç»œæ¨¡å‹ï¼Œå¦‚ ResNetã€VGG ç­‰ï¼Œä¸éœ€è¦æ‰‹åŠ¨åˆ›å»ºç½‘ç»œï¼Œå¯ä»¥ç›´æ¥ä» keras.applications å­æ¨¡å—ä¸­é€šè¿‡ä¸€è¡Œä»£ç å³å¯åˆ›å»ºå¹¶ä½¿ç”¨è¿™äº›ç»å…¸æ¨¡å‹ï¼ŒåŒæ—¶è¿˜å¯ä»¥é€šè¿‡è®¾ ç½® weights å‚æ•°åŠ è½½é¢„è®­ç»ƒçš„ç½‘ç»œå‚æ•°ï¼Œéå¸¸æ–¹ä¾¿ã€‚`



**å¯æ˜¯åŒ–**

`TensorBoard æ˜¯éœ€è¦å’Œæ¨¡å‹ä»£ç å’Œæµè§ˆå™¨ç›¸äº’é…åˆ`

`åœ¨æ¨¡å‹ç«¯ï¼Œé¦–å…ˆéœ€è¦å†™å…¥ç›‘æ§æ•°æ®çš„Summary ç±»ï¼Œå¹¶åœ¨éœ€è¦çš„æ—¶å€™å†™å…¥ç›‘æ§æ•°æ®`

```python
# åˆ›å»ºç›‘æ§ç±»ï¼Œç›‘æ§æ•°æ®å°†å†™å…¥ log_dir ç›®å½•
summary_writer = tf.summary.create_file_writer(log_dir)
```

`åœ¨å‰å‘è®¡ç®—å®Œ æˆåï¼Œå¯¹äºè¯¯å·®è¿™ç§æ ‡é‡æ•°æ®ï¼Œæˆ‘ä»¬é€šè¿‡ tf.summary.scalar å‡½æ•°è®°å½•ç›‘æ§æ•°æ®ï¼Œå¹¶æŒ‡å®šæ—¶ é—´æˆ³ step å‚æ•°`

```python
with summary_writer.as_default(): # å†™å…¥ç¯å¢ƒ
		# å½“å‰æ—¶é—´æˆ³ step ä¸Šçš„æ•°æ®ä¸º lossï¼Œå†™å…¥åˆ°åä¸º train-loss æ•°æ®åº“ä¸­
   tf.summary.scalar('train-loss', float(loss), step=step)
```

`å¯¹äºå›¾ç‰‡ç±»å‹çš„æ•°æ®ï¼Œå¯ä»¥é€šè¿‡ tf.summary.image å‡½æ•°å†™å…¥ç›‘æ§å›¾ç‰‡æ•°æ®`

```python
with summary_writer.as_default():# å†™å…¥ç¯å¢ƒ
	# å†™å…¥æµ‹è¯•å‡†ç¡®ç‡
	tf.summary.scalar('test-acc', float(total_correct/total),step=step)
	# å¯è§†åŒ–æµ‹è¯•ç”¨çš„å›¾ç‰‡ï¼Œè®¾ç½®æœ€å¤šå¯è§†åŒ– 9 å¼ å›¾ç‰‡
  tf.summary.image("val-onebyone-images:", val_images, max_outputs=9, step=step)
```

åœ¨æ¨¡å‹ä¸­å°†æ•°æ®å†™åˆ°ç£ç›˜ä¸­ä¹‹åï¼Œå€ŸåŠ©`Web æµè§ˆå™¨` å¯è§†åŒ–ï¼Œç„¶åä½¿ç”¨å‘½ä»¤

`tensorboard --logdir path` ç„¶åæ ¹æ®æ‰“å°å‡ºæ¥çš„infoï¼Œæ‰“å¼€å¯¹äºçš„è¿æ¥å¯è§†åŒ–ã€‚

TensorBoard è¿˜æ”¯æŒé€šè¿‡ tf.summary.histogram æŸ¥çœ‹

å¼ é‡æ•°æ®çš„ç›´æ–¹å›¾åˆ†å¸ƒï¼Œä»¥åŠé€šè¿‡ tf.summary.text æ‰“å°æ–‡æœ¬ä¿¡æ¯ç­‰åŠŸèƒ½

```python
with summary_writer.as_default():
		# å½“å‰æ—¶é—´æˆ³ step ä¸Šçš„æ•°æ®ä¸º lossï¼Œå†™å…¥åˆ° ID ä½ train-loss å¯¹è±¡ä¸­
		tf.summary.scalar('train-loss', float(loss), step=step) 
    # å¯è§†åŒ–çœŸå®æ ‡ç­¾çš„ç›´æ–¹å›¾åˆ†å¸ƒ 		
    tf.summary.histogram('y-hist',y, step=step)
		# æŸ¥çœ‹æ–‡æœ¬ä¿¡æ¯
		tf.summary.text('loss-text',str(float(loss)))
```







