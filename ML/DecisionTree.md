![aggregation](./pic/DecisionTree.png)

上面的内容是Adaboost 课程中已经学习过的内容，blending 是在学习到基模型之后去将模型融合，同时也有学习的方法，一边学习基模型，一边将基模型融合。DecisionTree 是另外的一种这样的方法。

![aggregation](./pic/DecisionTree1.png)

如上图右侧就是一颗决策树，数据经过每一个node然后落入到左右2侧，最后落入到最后的node中，在最后的node 中使用基模型g去判断最终数据的结果，一般而言这个基模型g是一个常量值(均值(回归问题)， 众数(分类问题))。

判断数据落入左侧还是右侧，是由条件函数q决策。

**决策树算法**

![aggregation](./pic/DecisionTree2.png)

所以上面有4个问题

* 选择多少个分支
* 如何选择分支
* 选择分支的时候，数据集拆分策略
* 基模型选择

![aggregation](./pic/DecisionTree3.png)

cart 树是决策时中比较具有代表性的模型，对解决上述的问题有一些独特的地方

* 每次切分的时候只是一刀切两半
* 一刀切两半，那么分支自然就是2个了
* 拆分策略有自己的函数
* 基模型返回一个常量值。回归问题返回落到节点上的均值，分类问题返回落到节点上叶子的众数那个类。

---

决策树可以看做是一组的 if-then 规则的集合。

简单只有2维特征的划分

![aggregation](./pic/DecisionTreeSplit.png)



决策树的学习的算法通常是一个递归的选择最优特征，并根据该特征训练数据进行分割，使得对各个数据及有一个最好的分类的过程，这个过程对于这特征空间的划分，也对应着决策树的构建，开始构建根节点，将所有训练数据都放在根节点，选择一个最优特征，安装这个特征将训练数据及分割成子集，使得各个子集有一个在当前条件下最好的分类。

如果这样一直划分下去对训练数据集能够做到完全的分类，但是对测试集就不一定了，也就是容易构成过拟合。所以需要剪枝。

一个决策树算法包括：

* 特征选择
* 决策树生成
* 决策树剪枝

决策树学习算法表示一个条件概率模型。

#### 特征选择

> 特征选择在于选取对数据集具有分类能力的特征，通常有信息增益或者增益比

如现在有数据：

![aggregation](./pic/featureSelection.png)

![aggregation](./pic/featureSelection1.png)

如上图是表示在年龄和有工作两个特征上的特征分列，那么应该选择哪个特征呢？ 选择一个特征应该是数据及在当前选择特性下有最好的分类。

通常有信息增益/比评价特征的好坏。

> **熵**
>
> 表示随机变量不确定性的度量，越大表示随机变量的不确定性越大

![aggregation](./pic/熵.png)

**条件熵**

![aggregation](./pic/熵1.png)

**信息增益**

![aggregation](./pic/熵2.png)

> H(D) 经验熵表示对数据及D进行分类的不确定性，H(D|A) 表示在特征A 下给定的条件下对数据及D进行分类的不确定性，那么他们的差，就表示在给定条件/特征下A 而使得对数据集D的分类不确定性减少的程度。

对上表中数据进行计算

![aggregation](./pic/gainCalc.png)

>  根据一个特征是如何最数据拆分的？
>
> 计算出值最大的那个特征，然后在这个特征下去选择一个值，大于这个值和小于这个值得两部分数据的label 能够做大最纯。查看cart的切分过程，就能够理解。

![aggregation](./pic/gainCalc1.png)

![aggregation](./pic/gini.png)

