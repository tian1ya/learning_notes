1. **spark on yarn 作业执行流程，yarn-client 和 yarn cluster 有什么区别**

   > **Spark On Yarn 的优势** 1. Spark 支持资源动态共享，运行于 Yarn 的框架都共享一个集中配置好的资源池 2. 可以很方便的利用 Yarn 的资源调度特性来做分类·，隔离以及优先级控制负载，拥有更灵活的调度策略 3. Yarn 可以自由地选择 executor 数量 4. Yarn 是唯一支持 Spark 安全的集群管理器（**Mesos???**），使用 Yarn，Spark 可以运行于 Kerberized Hadoop 之上，在它们进程之间进行安全认证
   >
   > **yarn-client 和 yarn cluster 的异同** 1. 从广义上讲，yarn-cluster 适用于生产环境。而 yarn-client 适用于交互和调试，也就是希望快速地看到 application 的输出。 2. 从深层次的含义讲，yarn-cluster 和 yarn-client 模式的区别其实就是 **Application Master 进程**的区别，yarn-cluster 模式下，driver 运行在 AM(Application Master)中，它负责向 YARN 申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉 Client，作业会继续在 YARN 上运行。然而 yarn-cluster 模式不适合运行交互类型的作业。而 yarn-client 模式下，Application Master 仅仅向 YARN 请求 executor，Client 会和请求的 container 通信来调度他们工作，也就是说 Client 不能离开。

2. Spark为什么快，Spark SQL 一定比 Hive 快吗

   > Spark SQL 比 Hadoop Hive 快，是有一定条件的，而且不是 Spark SQL 的引擎比 Hive 的引擎快，相反，Hive 的 HQL 引擎还比 Spark SQL 的引擎更快。其实，关键还是在于 Spark 本身快。
   >
   > 1. 消除了冗余的 HDFS 读写: Hadoop 每次 shuffle 操作后，必须写到磁盘，而 Spark 在 shuffle 后不一定落盘，可以 cache 到内存中，以便迭代时使用。如果操作复杂，很多的 shufle 操作，那么 Hadoop 的读写 IO 时间会大大增加，也是 Hive 更慢的主要原因了。
   > 2. 消除了冗余的 MapReduce 阶段: Hadoop 的 shuffle 操作一定连着完整的 MapReduce 操作，冗余繁琐。而 Spark 基于 RDD 提供了丰富的算子操作，且 reduce 操作产生 shuffle 数据，可以缓存在内存中。
   > 3. JVM 的优化: Hadoop 每次 MapReduce 操作，启动一个 Task 便会启动一次 JVM，**基于进程的操作**。而 Spark 每次 MapReduce 操作是基于线程的，只在启动 Executor 是启动一次 JVM，内存的 Task 操作是在线程复用的。每次启动 JVM 的时间可能就需要几秒甚至十几秒，那么当 Task 多了，这个时间 Hadoop 不知道比 Spark 慢了多少。
   >
   > **记住一种反例** 考虑一种极端查询:
   >
   > ```sql
   > Select month_id, sum(sales) from T group by month_id;
   > ```
   >
   > 这个查询只有一次 shuffle 操作，此时，也许 Hive HQL 的运行时间也许比 Spark 还快，反正 shuffle 完了都会落一次盘，或者都不落盘。
   >
   > **结论** Spark 快不是绝对的，但是绝大多数，Spark 都比 Hadoop 计算要快。这主要得益于其对 mapreduce 操作的优化以及对 JVM 使用的优化。

3. RDD, DAG, Stage怎么理解？

   > **DAG** Spark 中使用 DAG 对 RDD 的关系进行建模，描述了 RDD 的依赖关系，这种关系也被称之为 lineage（血缘），RDD 的依赖关系使用 Dependency 维护。DAG 在 Spark 中的对应的实现为 DAGScheduler。
   >
   > **RDD** RDD 是 Spark 的灵魂，也称为弹性分布式数据集。一个 RDD 代表一个可以被分区的、容错的只读数据集。RDD 内部可以有许多分区(partitions)，每个分区又拥有大量的记录(records)。Rdd的五个特征： 
   >
   > 1. dependencies: 建立 RDD 的依赖关系，主要 RDD 之间是宽窄依赖的关系，具有窄依赖关系的 RDD 可以在同一个 stage 中进行计算。 
   > 2. partition: 一个 RDD 会有若干个分区，分区的大小决定了对这个 RDD 计算的粒度，每个 RDD 的分区的计算都在一个单独的任务中进行。 
   > 3. preferedlocations: 按照“移动数据不如移动计算”原则，在 Spark 进行任务调度的时候，优先将任务分配到数据块存储的位置。 
   > 4. compute: Spark 中的计算都是以分区为基本单位的，compute 函数只是对迭代器进行复合，并不保存单次计算的结果。 
   > 5. partitioner: 只存在于（K,V）类型的 RDD 中，非（K,V）类型的 partitioner 的值就是 None。
   >
   > RDD 的算子主要分成2类，action 和 transformation。这里的算子概念，可以理解成就是对数据集的变换。action 会触发真正的作业提交，而 transformation 算子是不会立即触发作业提交的。每一个 transformation 方法返回一个新的 RDD。只是某些 transformation 比较复杂，会包含多个子 transformation，因而会生成多个 RDD。这就是实际 RDD 个数比我们想象的多一些 的原因。通常是，当遇到 action 算子时会触发一个job的提交，然后反推回去看前面的 transformation 算子，进而形成一张有向无环图。
   >
   > **Stage** 在 DAG 中又进行 stage 的划分，划分的依据是依赖是否是 shuffle 的，每个 stage 又可以划分成若干 task。接下来的事情就是 driver 发送 task 到 executor，**executor 自己的线程池**去执行这些 task，完成之后将结果返回给 driver。action 算子是划分不同 job 的依据。

4. RDD 如何通过记录更新的方式容错

   >  RDD 的容错机制实现分布式数据集容错方法有两种: 1. 数据检查点 2. 记录更新。
   >
   > RDD 采用记录更新的方式：**记录所有更新点**的成本很高。所以，RDD只支持粗颗粒变换，即只记录单个块（分区）上执行的单个操作，然后创建某个 RDD 的变换序列（血统 lineage）存储下来；变换序列指，每个 RDD 都包含了它是如何由其他 RDD 变换过来的以及如何重建某一块数据的信息。因此 RDD 的容错机制又称“血统”容错。

5. 宽依赖、窄依赖怎么理解？

   > 1. 窄依赖指的是每一个 parent RDD 的 partition 最多被子 RDD 的一个 partition 使用（一子一亲）
   > 2. 宽依赖指的是一个子 RDD 的 partition 会依赖多个 parent RDD的 partition（多亲一子）
   >
   > RDD 作为数据结构，本质上是一个**只读的分区记录集合**。一个 RDD 可以包含多个分区，每个分区就是一个 dataset 片段。RDD 可以相互依赖。
   >
   > 首先，窄依赖可以支持在同一个 cluster node上，以 pipeline 形式执行多条命令（也叫同一个 stage 的操作），例如在执行了 map 后，紧接着执行 filter。相反，宽依赖需要所有的父分区都是可用的，可能还需要调用类似 MapReduce 之类的操作进行跨节点传递。
   >
   > 其次，则是从失败恢复的角度考虑。窄依赖的失败恢复更有效，因为它只需要重新计算丢失的 parent partition 即可，而且可以并行地在不同节点进行重计算（一台机器太慢就会分配到多个节点进行），相反，宽依赖牵涉 RDD 各级的多个 parent partition。

6. Spark 血统的概念

   > RDD 的 lineage 记录的是**粗颗粒度**的特定数据转换（transformation）操作（filter, map, join etc.)行为。当这个 RDD 的部分分区数据丢失时，它可以通过 lineage 获取足够的信息来重新运算和恢复丢失的数据分区。这种粗颗粒的数据模型，限制了 Spark 的运用场合，但同时相比细颗粒度的数据模型，也带来了性能的提升。

7. 容错方法

   > Spark 选择记录更新的方式。但是，如果更新粒度太细太多，那么记录更新成本也不低。因此，RDD只支持粗粒度转换，即**只记录单个块上执行的单个操作**，然后将创建 RDD 的一系列变换序列（每个 RDD 都包含了他是如何由其他 RDD 变换过来的以及如何重建某一块数据的信息。因此 RDD 的容错机制又称血统容错）记录下来，以便恢复丢失的分区。lineage本质上很类似于数据库中的重做日志（Redo Log），只不过这个重做日志粒度很大，是对全局数据做同样的重做进而恢复数据。
   >
   > 相比其他系统的细颗粒度的内存数据更新级别的备份或者 LOG 机制，RDD 的 lineage 记录的是粗颗粒度的特定数据 transformation 操作行为。当这个 RDD 的部分分区数据丢失时，它可以通过 lineage 获取足够的信息来重新运算和恢复丢失的数据分区。

8. spark  优越性

   > 一、Spark 的5大优势： 
   >
   > 1. 更高的性能。因为数据被加载到集群主机的分布式内存中。数据可以被快速的转换迭代，并缓存用以后续的频繁访问需求。在数据全部加载到内存的情况下，Spark可以比Hadoop快100倍，在内存不够存放所有数据的情况下快hadoop10倍。 
   > 2. 通过建立在Java,Scala,Python,SQL（应对交互式查询）的标准API以方便各行各业使用，同时还含有大量开箱即用的机器学习库。 
   > 3. 与现有Hadoop 1和2.x(YARN)生态兼容，因此机构可以无缝迁移。 
   > 4. 方便下载和安装。方便的shell（REPL: Read-Eval-Print-Loop）可以对API进行交互式的学习。 
   > 5. 借助高等级的架构提高生产力，从而可以讲精力放到计算上。
   >
   > 二、MapReduce与Spark相比，有哪些异同点：
   >
   >  1、基本原理上： 
   >
   > ​	（1） MapReduce：基于磁盘的大数据批量处理系统 
   >
   > ​	（2）Spark：基于RDD(弹性分布式数据集)数据处理，显示将RDD数据存储到磁盘和内存中。 
   >
   > 2、模型上： 
   >
   > ​	（1） MapReduce可以处理超大规模的数据，适合日志分析挖掘等较少的迭代的长任务需求，结合了数据的分布式的计算。 
   > ​	（2） Spark：适合数据的挖掘，机器学习等多轮迭代式计算任务。

9. spark作业提交流程是怎么样的

   > 1. `spark-submit` 提交代码，执行 `new SparkContext()`，在 SparkContext 里构造 `DAGScheduler` 和 `TaskScheduler`。
   > 2. TaskScheduler 会通过后台的一个进程，连接 Master，向 Master 注册 Application。
   > 3. Master 接收到 Application 请求后，会使用相应的资源调度算法，在 Worker 上为这个 Application 启动多个 Executer。
   > 4. Executor 启动后，会自己反向注册到 TaskScheduler 中。 所有 Executor 都注册到 Driver 上之后，SparkContext 结束初始化，接下来往下执行我们自己的代码。
   > 5. 每执行到一个 Action，就会创建一个 Job。Job 会提交给 DAGScheduler。
   > 6. DAGScheduler 会将 Job划分为多个 stage，然后每个 stage 创建一个 TaskSet。
   > 7. TaskScheduler 会把每一个 TaskSet 里的 Task，提交到 Executor 上执行。
   > 8. Executor 上有线程池，每接收到一个 Task，就用 TaskRunner 封装，然后从线程池里取出一个线程执行这个 task。(TaskRunner 将我们编写的代码，拷贝，反序列化，执行 Task，每个 Task 执行 RDD 里的一个 partition)

10. Spark 的 ML 和 MLLib 两个包区别和联系

    > 1. 技术角度上，面向的数据集类型不一样: ML 的 API 是面向 Dataset 的（Dataframe 是 Dataset 的子集，也就是 Dataset[Row]）， mllib 是面对 RDD 的。Dataset 和 RDD 有啥不一样呢？Dataset 的底端是 RDD。Dataset 对 RDD 进行了更深一层的优化，比如说有 sql 语言类似的黑魔法，Dataset 支持静态类型分析所以在 compile time 就能报错，各种 combinators（map，foreach 等）性能会更好，等等。
    > 2. 编程过程上，构建机器学习算法的过程不一样: ML 提倡使用 pipelines，把数据想成水，水从管道的一段流入，从另一端流出。ML 是1.4比 Mllib 更高抽象的库，它解决如果简洁的设计一个机器学习工作流的问题，而不是具体的某种机器学习算法。未来这两个库会并行发展。 

11. 为什么要用Yarn来部署Spark?

    > 因为 Yarn 支持动态资源配置。Standalone 模式只支持简单的固定资源分配策略，每个任务固定数量的 core，各 Job 按顺序依次分配在资源，资源不够的时候就排队。这种模式比较适合单用户的情况，多用户的情境下，会有可能有些用户的任务得不到资源。
    >
    > Yarn 作为通用的种子资源调度平台，除了 Spark 提供调度服务之外，还可以为其他系统提供调度，如 Hadoop MapReduce, Hive 等。 

12. 说说 persist() 和 cache() 的异同

    > ```scala
    > cache() = persist(StorageLevel.MEMORY_ONLY)
    > ```

13. 说说下面代码的异同

    > ```scala
    > val counter = 0
    > val data = Seq(1, 2, 3)
    > data.foreach(x => counter += x)
    > println("Counter value: " + counter)
    > 
    > val counter = 0
    > val data = Seq(1, 2, 3)
    > var rdd = sc.parallelizze(data)
    > rdd.foreach(x => counter += x)
    > println("Counter value: " + counter)
    > ```
    >
    > 所有在 Driver 程序追踪的代码看上去好像在 Driver 上计算，实际上都不在本地，每个 RDD 操作都被转换成 Job 分发至集群的执行器 Executor 进程中运行，即便是单机本地运行模式，也是在单独的执行器进程上运行，与 Driver 进程属于不用的进程。所以每个 Job 的执行，都会经历序列化、网络传输、反序列化和运行的过程。
    >
    > 再具体一点解释是 foreach 中的匿名函数 x => counter += x 首先会被序列化然后被传入计算节点，反序列化之后再运行，因为 foreach 是 Action 操作，结果会返回到 Driver 进程中。
    >
    > 在序列化的时候，Spark 会将 Job 运行所依赖的变量、方法全部打包在一起序列化，相当于它们的副本，所以 counter 会一起被序列化，然后传输到计算节点，是计算节点上的 counter 会自增，而 Driver 程序追踪的 counter 则不会发生变化。执行完成之后，结果会返回到 Driver 程序中。而 Driver 中的 counter 依然是当初的那个 Driver 的值为0。
    >
    > 因此说，RDD 操作不能嵌套调用，即在 RDD 操作传入的函数参数的函数体中，不可以出现 RDD 调用。

14. 说说Spark提供的两种共享变量

    > Spark 程序的大部分操作都是 RDD 操作，通过传入函数给 RDD 操作函数来计算，这些函数在**不同的节点**上并发执行，内部的变量有不同的作用域，不能相互访问，有些情况下不太方便。
    >
    > 1. 广播变量，是一个只读对象，在所有节点上都有一份缓存，创建方法是 SparkContext.broadcast()。创建之后再更新它的值是没有意义的，一般用 val 来修改定义。
    > 2. 计数器，只能增加，可以用计数或求和，支持自定义类型。创建方法是 SparkContext.accumulator(V, name)。**只有 Driver 程序可以读这个计算器的变量**，RDD 操作中读取计数器变量是无意义的。但节点可以对该计算器进行增加

[以上的内容是该篇博客看到的](https://zhuanlan.zhihu.com/p/49169166)

---

#### sparkStreaming 的面试题

1. SparkStreaming 有几种消费kafka 中的数据的方式，他们之间的区别是什么

> 有2中，分别是经过Receiver 的方式，另外一种是通过直连的方式
>
> Receiver 的方式是通过 kafka 的高层API 实现的，读过来的数据会存储在`Executor` 中，然后在启动`Job` 从`Executor` 中消费这些数据，在这个过程中为了实现不丢失数据 `sparkStreaming` 这边也会去写一些日志(WAL)即使是kafka 保证数据不丢失，这种方式效率低下，在 `sparkStreaming` 和 `kafka` 端存储2套日志，这些日志可能会不限不同步。且读数据的时候频繁的读取高级`API` 中的数据，造车效率低下`(zookeeper)`，不适合做频繁的读取数据。
>
> 还有一种直连的方式，直接通过 `kafka` 底层的数据，对接到主题下的分区，`kafka`有几个分区那么读进来 `spark` 中就有几个分区数据，更加高效。需要 `sparkStreaming` 自己维护偏移。

2. spark Streaming 的窗口原理

> spark STREAMING  的窗口函数是原来定义批次的大小(时间间隔)基础之上，再次封装，每个窗口中的包含着若干批的数据，不管是窗口大小，还是步长，都需要是批大小的整数倍。
>
> 在每一批数据之上实现各种的聚合函数，这个时候和操作批中的rdd 基本是一致的，不同的是各个窗口之间可能会存在重复的数据，针对这些重复的数据，也是可以传一个函数进去。

3. **spark streaming的一个特点就是高容错**

> ​		首先spark rdd就有容错机制,每一个rdd都是不可变的分布式可重算的数据集,其记录这确定性的**操作血统**,所以只要输入数据是可容错的,那么任意一个rdd的分区出错或不可用,都是可以利用原始输入数据通过转换操作而重新计算的
>
> ​		预写日志通常被用于数据库和文件系统中,保证数据操作的持久性.预写日志通常是先将操作写入到一个持久可靠的日志文件中,然后才对数据施加该操作,当加入施加操作中出现了异常,可以通过读取日志文件并重新施加该操作
>
> ​		另外接收数据的正确性只在数据被预写到日志以后接收器才会确认,已经缓存但还没保存的数据可以在driver重新启动后由数据源再发送一次,这两个机制确保了零数据丢失,所有数据或从日志中恢复或者由数据源重发.
>
> ​	状态的checkpoint存储，在sparkStreamingContext中设置一个checkpoint 的路径，可以在重启sparkStreamingContext 的时候读取这个路径下的已经存储的状态，基于该状态重新开始后续计算。
>
> kafkaRDD，在一开始消费的时候可以给一个偏移量，基于该偏移量继续消费，这偏移量可以是出错之前储存在外部事务数据库中的偏移，在重启的时候，重新读进来，然后继续上次的地方消费。

